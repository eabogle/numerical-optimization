{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a0d222-45d5-43f5-95a1-de1755752d59",
   "metadata": {},
   "source": [
    "### MTH 610 Model Constrained Optimization 2 (Homework 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f350870c-26b0-4f2f-ab1d-d340e9d2ad47",
   "metadata": {},
   "source": [
    "#### Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8217f5f-e6e4-4f18-b480-a92e03303097",
   "metadata": {},
   "source": [
    "This assignment investigates the ability of the observation sensitivity apporahc to provide an apriori estimate to the impact of data removal in a model-constrained optimization problem. The setup is based on the tools developed in the previous homework (Homework_4) assignment as follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f36b6-8c22-4b2a-8ed9-0fcebbf76110",
   "metadata": {},
   "source": [
    "Consider the boundary value problem (BVP) for a function $x:[0,1] \\to \\mathbb{R}$,\n",
    "$$\n",
    "\\begin{cases}\n",
    "-x''(t)+x^3(t)=u\\\\\n",
    "x(0)=0, \\quad x(1)=0\n",
    "\\end{cases}\n",
    "\\tag{1}\n",
    "$$\n",
    "where $u\\in\\mathbb{R}$ is a constant scalar time-independent parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0008c-0e2b-4582-b173-188318a18a6e",
   "metadata": {},
   "source": [
    "A discrete version to the original model BVP is obtained by considering a uniform partition of the interval $[0,1]$ with nodes $\\{t_i:i=0:n+1\\}$ at an increment $h=\\frac{1}{n+1}$,\n",
    "$$\n",
    "0=t_0<t_1<\\dots<t_n<t_{n+1}=1, \\quad t_i=i*h \\textrm{ for } i=0:n+1\n",
    "$$\n",
    "and an approximation for the second order derivative using a finite difference formula is \n",
    "$$\n",
    "x''(t_i)\\approx\\frac{x(t_{i+1})-2x(t_i)+x(t_{i-1})}{h^2}, \\quad i=1:n\n",
    "$$\n",
    "\n",
    "Therefore a discrete version of the BVP is given by,\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{x_{i+1}-2x_i+x_{i-1}}{h^2} + x^3_{i} = u_i, \\quad i=1:n\\\\\n",
    "x_0=0, \\quad x_{n+1}=0\n",
    "\\end{cases}\n",
    "$$\n",
    "where $x_i\\approx x(t_i), \\quad i=1:n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d437f6-d5b8-41ae-a49e-383608539909",
   "metadata": {},
   "source": [
    "We can write this sytem of equations in matrix vector format,\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}+\\mathbf{G}(\\mathbf{x})=u\\mathbf{1_N}\n",
    "\\tag{2}\n",
    "$$\n",
    "where $u \\in \\mathbb{R}$ is a scalar, $\\mathbf{1_N}\\in \\mathbb{R}^N$ is the constant vector of 1's, $\\mathbf{A}\\in \\mathbb{R}^{N*N}$ is a scaled tridaigonal matrix with 2 on the main diagonal and -1 on the neighboring minor diagonals and the scaling constant is given by $\\frac{1}{h^2}$, and $\\mathbf{G}:\\mathbb{R}^N\\to\\mathbb{R}^N$ is a vector valued funcition with $x_i^3$ in each component. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e46bc2-f9ff-4fc4-80bb-ba644adf2eb5",
   "metadata": {},
   "source": [
    "Let $y\\in \\mathbb{R^2}$ denote the vector with components $y_i=\\sin(t_i\\pi), i =1:n$ and let $(\\mathbf{x}^*, u^*)\\in \\mathbb{R}^n \\times \\mathbb{R}$ denote the solution to the optimization problem \n",
    "\\begin{cases}\n",
    "\\min J \\ \\dot{=} \\ \\frac{h}{2} ||\\mathbf{x}-\\mathbf{y}||^2=\\frac{h}{2}\\sum_{j=1}^n (x_j-y_j)^2 \\\\\n",
    "\\textrm{subject to constraints (2)}\n",
    "\\tag{3}\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383c081-6d78-4adb-82ce-d8b9252b5f65",
   "metadata": {},
   "source": [
    "Let $(\\mathbf{x}_{[i]}^*, u_{[i]}^*)\\in \\mathbb{R}^n\\times \\mathbb{R}$ denote the solution to the optimization problem obtained by removing the componenet $y_i$ from the data vector $\\mathbf{y}$ i.e., by performing an observing system experiment (OSE)\n",
    "\\begin{cases}\n",
    "\\min J_{[i]} \\ \\dot{=} \\ \\frac{h}{2}\\sum_{j=1, j\\ne i}^n(x_j-y_j)^2\\\\\n",
    "\\textrm{subject to constraints (2)}\n",
    "\\tag{4}\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b3e04-b627-4a19-ad82-4e0368323096",
   "metadata": {},
   "source": [
    "Now, in an equivalent fashion we can introduce a vector $\\mathbf{s}$ where $\\mathbf{\\bar{s}}=\\mathbf{1}^n$, and $\\mathbf{s}_{[i]}=\\mathbf{1}^n$ with $0$ in the $i$-th component. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41183ba9-587d-4d8b-87b5-d7e88f25e4af",
   "metadata": {},
   "source": [
    "In doing so, the process of removing a data component can be achieved by multiplying the cost function in (4) by $s_i$. That is to say, we can rewrite (4) in the following way:\n",
    "\\begin{cases}\n",
    "\\min J_{[i]} \\ \\dot{=} \\ \\frac{h}{2}\\sum_{j=1}^n(x_j-y_j)^2s_j\\\\\n",
    "\\textrm{subject to constraints (2)}\n",
    "\\tag{5}\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b3314-c2b3-403d-98d2-513840d23464",
   "metadata": {},
   "source": [
    "In general this cost function is of the form:\n",
    "\\begin{cases}\n",
    "\\min J \\ \\dot{=} \\ \\frac{h}{2}||\\mathbf{x}-\\mathbf{y}||^2\\mathbf{s}\\\\\n",
    "\\textrm{subject to constraints (2)}\n",
    "\\tag{5}\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15098f63-1f6b-49ba-8c26-2835666dd52c",
   "metadata": {},
   "source": [
    "But, we also know that from the previous assignment that $\\mathbf{x}$ can be implicitly defined in terms of $u$ therefore in general the cost function is of the form:\n",
    "\\begin{cases}\n",
    "\\min J \\ \\dot{=} \\  \\frac{h}{2}||\\mathbf{x}(u)-\\mathbf{y}||^2\\mathbf{s}\\\\\n",
    "\\textrm{subject to constraints (2)}\n",
    "\\tag{6}\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5f96a-ec74-4152-b53b-73a6aeb87e35",
   "metadata": {},
   "source": [
    "#### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131439bf-a387-4385-9fd8-7b80492eb0ab",
   "metadata": {},
   "source": [
    "A qanitity of interest (qoi, forcast aspect) is specified in terms of the state vector $\\mathbf{x}$ as\n",
    "$$\n",
    "q(\\mathbf{x})=\\frac{1}{2}||\\mathbf{x}||^2\n",
    "\\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b1ae2-6671-4064-a7f9-2f432309f48a",
   "metadata": {},
   "source": [
    "The imact of the qoi produced by removing any individual data component $y_i, i=1:n$ from the optimization problem (3) is defined as\n",
    "$$\n",
    "\\Delta q_i \\ \\dot{=} \\ q(\\mathbf{x}^*_{[i]}-q(\\mathbf{x}^*))=\\frac{1}{2}||\\mathbf{x}_{[i]}^*||^2-\\frac{1}{2}||\\mathbf{x}^*||^2\n",
    "\\tag{8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeeed15-13a4-417c-a2c2-c826c0d977da",
   "metadata": {},
   "source": [
    "For each data component $y_i, i=1:n$, and exact evlautaion to (7) may be obtained by performing the coresponding OSE (4),(5). The OSE apporach requires solving an additional optimization problem for each data component and in practice, it may become computationally prohibitive if the dimension of the data vector is large. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da34c95-4bb6-4661-b68a-8a454760a93c",
   "metadata": {},
   "source": [
    "Now we note that\n",
    "$$\n",
    "q(\\mathbf{x})=q(\\mathbf{x}(u(\\mathbf{s})))=\\tilde{q}(\\mathbf{s})\n",
    "\\tag{9}\n",
    "$$\n",
    "Furthermore, $q(\\mathbf{x}_{[i]}^*)=\\tilde{q}(\\mathbf{s_{[i]}})$ and $q(\\mathbf{x}^*)=\\tilde{q}(\\mathbf{\\bar{s}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeceaf6-e88c-40dd-a819-e034bb9f2e63",
   "metadata": {},
   "source": [
    "Evaluation of the forecase sensitivity to observations (FSO) vector\n",
    "$$\n",
    "\\nabla_\\mathbf{s}q(\\mathbf{x}^*)\n",
    "\\tag{10}\n",
    "$$\n",
    "in the status quo configuration,\n",
    "\\begin{cases}\n",
    "\\min J \\ \\dot{=} \\ \\frac{h}{2} ||\\mathbf{x}-\\mathbf{y}||^2\\mathbf{\\bar{s}}=\\frac{h}{2}\\sum_{j=1}^n (x_j-y_j)^2\\mathbf{\\bar{s}} \\\\\n",
    "\\textrm{subject to constraints (2)}\n",
    "\\tag{11}\n",
    "\\end{cases}\n",
    "provides a computationally efficient appraoch (all at once) to obtain apriori a first-order approximation to the impact of removing any individual data component,\n",
    "$$\n",
    "\\tilde{q}(\\mathbf{s}_{[i]}^*)-\\tilde{q}(\\mathbf{\\bar{s}})\\approx \\delta q_i \\ \\dot{=} \\ (\\mathbf{s}_{[i]})-\\mathbf{\\bar{s}})^T\\nabla_s \\tilde{q}(\\mathbf{\\bar{s}}) = -[\\nabla_s\\tilde{q}(\\mathbf{\\bar{s}})]_{[i]}\n",
    "\\tag{12}\n",
    "$$\n",
    "(by first order Taylor expansion around $\\mathbf{\\bar{s}}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460142fd-31a8-4095-aef3-88b9dcdc9bf7",
   "metadata": {},
   "source": [
    "Therefore we need to find $[\\nabla_s\\tilde{q}(\\mathbf{\\bar{s}})]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd030d-92ee-4c23-b963-18fe821a39b0",
   "metadata": {},
   "source": [
    "We know \n",
    "\\begin{align*}\n",
    "\\nabla_s\\tilde{q}&=\\nabla_s q(\\mathbf{x}(u(\\mathbf{s})))\\\\\n",
    "&= \\nabla_{\\mathbf{s}}u \\nabla_u \\mathbf{x} \\nabla_{\\mathbf{x}} q\\\\\n",
    "&= \\nabla_{\\mathbf{s}}u \\biggl[ \\frac{\\partial \\mathbf{x}}{\\partial u}\\biggr]^T\\mathbf{x}\\\\\n",
    "\\tag{13}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89389fb-9d4f-4015-96b8-16db143ab665",
   "metadata": {},
   "source": [
    "We will evaluate this in the reference system in (11) at optimality $\\mathbf{x}^*$. Now we already know from our previous work that $\\biggl[ \\frac{\\partial \\mathbf{x}}{\\partial u}\\biggr]^T$ can be found from the TLM and is given by $$\n",
    "\\biggl[ \\frac{\\partial \\mathbf{x}}{\\partial u}\\biggr]^T=[([\\mathbf{A}]+[G_x])^{-1}\\mathbf{1}^n]^T \\in \\mathbb{R}^{1 \\times n}\\\\\n",
    "\\implies \\biggl[ \\frac{\\partial \\mathbf{x}}{\\partial u}\\biggr]=[([\\mathbf{A}]+[G_x])^{-1}\\mathbf{1}^n] \\in \\mathbb{R}^{n}\n",
    "\\tag{14}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b4dba-dcf1-403b-a061-4fb51d9979a7",
   "metadata": {},
   "source": [
    "Furthermore, we need to find $\\nabla_{\\mathbf{s}}u$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b52181-0ba6-4c64-84ac-a9b584c20637",
   "metadata": {},
   "source": [
    "Since we are going to evaluate (13) at optimality we are going to assume FONC and SONC hold:\n",
    "$$\n",
    "\\nabla_u \\tilde{J}(u,\\mathbf{s}) = 0\n",
    "\\tag{FONC}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{uu}^2 \\tilde{J}(u,\\mathbf{s}) \\ge 0\n",
    "\\tag{SONC}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a0249-03ca-49a9-9daa-d10042ae18f7",
   "metadata": {},
   "source": [
    "Now, \n",
    "\\begin{align}\n",
    "\\nabla_u \\tilde{J}(u,\\mathbf{s}) &= \\nabla_u u \\nabla_u \\tilde{J} + \\nabla_u \\mathbf{s} \\nabla_{\\mathbf{s}} \\tilde{J}\\\\\n",
    "&= \\nabla_u \\tilde{J}\n",
    "\\tag{15}\n",
    "\\end{align}\n",
    "Therfore,\n",
    "\\begin{align}\n",
    "\\nabla_{su}\\tilde{J}(u,\\mathbf{s}) &= \\nabla_{\\mathbf{s}}u \\nabla_{uu}^2 \\tilde{J}+\\nabla_{\\mathbf{s}}\\mathbf{s} \\nabla_{\\mathbf{s}u}^2 \\tilde{J}\\\\\n",
    "&= \\nabla_{\\mathbf{s}}u \\nabla_{uu}^2 \\tilde{J}+ \\nabla_{\\mathbf{s}u}^2 \\tilde{J}\n",
    "\\tag{16}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b43aab-38b8-42ef-99bc-d2bbb23d2066",
   "metadata": {},
   "source": [
    "By (FONC) we have\n",
    "$$\n",
    "\\nabla_{su}\\tilde{J}(u,\\mathbf{s})=\\nabla_{\\mathbf{s}}u \\nabla_{uu}^2 \\tilde{J}+ \\nabla_{\\mathbf{s}u}^2 \\tilde{J} =0\\\\\n",
    "\\implies \\nabla_{\\mathbf{s}}u=-[\\nabla_{\\mathbf{s}u}^2 \\tilde{J}][\\nabla_{uu}^2 \\tilde{J}]^{-1}\n",
    "\\tag{17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9510ae-83c5-412f-93f9-777978adfc8d",
   "metadata": {},
   "source": [
    "Therfore, we need to find $\\nabla_{\\mathbf{s}u}^2 \\tilde{J}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54278f06-4a85-40e3-a766-c8d24910f763",
   "metadata": {},
   "source": [
    "We know\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{s}}\\tilde{J}(u,\\mathbf{s})&=\\nabla_{\\mathbf{s}}\\biggl[\\frac{h}{2}||\\mathbf{x}(u)-\\mathbf{y}||^2\\mathbf{s}\\biggr]\\\\\n",
    "&=\\frac{1}{2}||\\mathbf{x}(u)-\\mathbf{y}||^2\n",
    "\\tag{18}\n",
    "\\end{align}\n",
    "Therefore,\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{s}u}^2\\tilde{J}(u, \\mathbf{s}) &= \\nabla_u\\biggl[\\frac{1}{2}||\\mathbf{x}(u)-\\mathbf{y}||^2 \\biggr]\\\\\n",
    "&= h(\\mathbf{x}(u)-\\mathbf{y})\\odot\\biggl[\\frac{\\partial \\mathbf{x}}{\\partial u}\\biggr]\n",
    "\\tag{19}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1c0e7-594f-4dd9-a77b-7113a81fb3d9",
   "metadata": {},
   "source": [
    "Where the notation $\\odot$ means element wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c9424-5ce7-4e79-a2bb-e73afbbaa93f",
   "metadata": {},
   "source": [
    "Therefore, combining what we know from (17) and (19) we have:\n",
    "$$\n",
    "\\nabla_{\\mathbf{s}}u=-\\biggl[h(\\mathbf{x}(u)-\\mathbf{y})\\odot\\biggl[\\frac{\\partial \\mathbf{x}}{\\partial u}\\biggr]\\biggr][\\nabla_{uu}^2\\tilde{J}]^{-1} \\in \\mathbb{R}^{n}\n",
    "\\tag{20}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4fa783-7031-454d-ac77-91ebdb40919b",
   "metadata": {},
   "source": [
    "Therefore combining information from (20) into (13) within our reference system at optimality we have:\n",
    "$$\n",
    "\\nabla_{\\mathbf{s}}\\tilde{q}(\\mathbf{x^*}) = -\\biggr[h(\\mathbf{x}^*-\\mathbf{y})\\odot\\biggl[\\frac{\\partial \\mathbf{x}}{\\partial u}\\biggr]\\biggr][\\nabla_{uu}^2\\tilde{J}]^{-1} \\biggl[\\frac{\\partial \\mathbf{x}}{\\partial u}\\biggr]^T\\mathbf{x}^* \\in \\mathbb{R}^{n}\n",
    "\\tag{21}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0ac9b-9917-4796-a6a3-c48a079cff6f",
   "metadata": {},
   "source": [
    "Finally, we are able to get a final expression for the apriori a first-order approximation to the impact of removing any individual data component given in (12),\n",
    "\\begin{align} \n",
    "\\tilde{q}(\\mathbf{s}_{[i]}^*)-\\tilde{q}(\\mathbf{\\bar{s}})\\approx \\delta q_i \\ &\\dot{=} \\ (\\mathbf{s}_{[i]})-\\mathbf{\\bar{s}})^T\\nabla_s \\tilde{q}(\\mathbf{\\bar{s}})\\\\\n",
    "&= -[\\nabla_s\\tilde{q}(\\mathbf{\\bar{s}})]_{[i]}\\\\\n",
    "&= \\Biggl[h(\\mathbf{x}^*-\\mathbf{y})\\odot\\biggl[\\frac{\\partial \\mathbf{x}}{\\partial u}\\biggr][\\nabla_{uu}^2\\tilde{J}]^{-1} \\biggl[\\frac{\\partial \\mathbf{x}}{\\partial u}\\biggr]^T\\mathbf{x}^*\\Biggr]_{[i]}\n",
    "\\tag{22}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763b8a5-1ea1-452b-8565-1aa768d38c02",
   "metadata": {},
   "source": [
    "We will now provide the needed code to evaluate the sensitivity to the observation vector (10) and the observation impact estimates (12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19480bc2-8deb-4195-9bd2-37dbdc539e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import needed packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sympy\n",
    "from scipy.optimize import fsolve\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20e63b0-d61e-49f4-b36d-2f4a197638db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def homework5(uguess, step_size):\n",
    "    # Define constants\n",
    "    N=99\n",
    "    h=1/(N+1)\n",
    "    maxiter = 100000\n",
    "    tol = 1e-8\n",
    "    iteration = 0\n",
    "    \n",
    "    # Define A, the (discrete Laplace) matrix\n",
    "    main_diag = 2 * np.eye(N-1)\n",
    "    minor_diags = -1 * (np.eye(N-1, k=-1) + np.eye(N-1, k=1))\n",
    "    A = 1/(h*h) * (main_diag + minor_diags)\n",
    "    \n",
    "    #################### Functions ######################\n",
    "\n",
    "    # Define vector G(x), G_x, [G_xlamb]_x\n",
    "    def G(x):\n",
    "        return np.power(x,3)\n",
    "    \n",
    "    def G_x(x):\n",
    "        diagonal = 3 * np.power(x, 2)\n",
    "        return np.diag(diagonal)\n",
    "    \n",
    "    def G_xlamb_x(x, lamb):\n",
    "        diagonal = 6 * x * lamb\n",
    "        return np.diag(diagonal)\n",
    "    \n",
    "    #Define model equations to be solved or already in solved format\n",
    "    def xequation(x, A, u):\n",
    "        return A @ x + G(x) - u\n",
    "\n",
    "    def lambsolved(A, x, y, h):\n",
    "        return h * inv((A + G_x(x))) @ (x - y)\n",
    "    \n",
    "    def xisolved(A, x):\n",
    "        return inv(A + G_x(x)) @ np.ones(N-1) \n",
    "    \n",
    "    def etasolved(A, x, xi, lamb):\n",
    "        return inv(A + G_x(x))@(h * np.eye(N-1) - G_xlamb_x(x,lamb)) @ xi\n",
    "\n",
    "    # Solving models\n",
    "    def solvex(x_guess, A, smalluvect):\n",
    "        resultx = fsolve(xequation, x_guess, args=(A, smalluvect), xtol=1e-10)\n",
    "        solx = np.zeros(N+1)\n",
    "        solx[1:-1] = resultx[:]\n",
    "        return(resultx, solx)\n",
    "    \n",
    "    def solvelamb(A, resultx, smallyvect, h):\n",
    "        resultlamb = lambsolved(A, resultx, smallyvect, h)\n",
    "        sollamb = np.zeros(N+1)\n",
    "        sollamb[1:-1] = resultlamb[:]\n",
    "        return(resultlamb, sollamb)\n",
    "    \n",
    "    def solvexi(A, resultx):\n",
    "        resultxi = xisolved(A, resultx)\n",
    "        solxi = np.zeros(N+1)\n",
    "        solxi[1:-1] = resultxi[:]\n",
    "        return(resultxi)\n",
    "    \n",
    "    def solveeta(A, resultx, resultxi, resultlamb):\n",
    "        resulteta = etasolved(A, resultx, resultxi, resultlamb)\n",
    "        soleta = np.zeros(N+1)\n",
    "        soleta[1:-1] = resulteta[:]\n",
    "        return(soleta)\n",
    "    \n",
    "    # Define J(u), J'(u), J''(u) function value after you have x based on choice of u\n",
    "    \n",
    "    def jpufun(sollamb):\n",
    "        return np.dot(sollamb, np.ones(N+1))\n",
    "    \n",
    "    def jppufun(soleta):\n",
    "        return np.dot(soleta, np.ones(N+1))\n",
    "    \n",
    "    ################# Data/Vectors Intialization ##################\n",
    "\n",
    "    t_nodes = np.zeros(N+1)\n",
    "    yvect = np.zeros(N+1)\n",
    "    u_guess = uguess\n",
    "    u_guess_vect = u_guess * np.ones(N-1)\n",
    "    s_vect = np.ones(N+1)\n",
    "    x_guess = np.zeros(N-1)\n",
    "    osex = []\n",
    "    oseu = []\n",
    "\n",
    "    # Create y(t) vector based off y(t) = sin(pi*t)\n",
    "    def yfun(t):\n",
    "        return np.sin(np.pi * t)\n",
    "\n",
    "    for i in range(N+1):\n",
    "        t_nodes[i] = i*h\n",
    "        yvect[i] = yfun(t_nodes[i])\n",
    "        \n",
    "    smallyvect = yvect[1:-1]\n",
    "    \n",
    "    #################### Optimization ######################\n",
    "    \n",
    "    # Find first gradient value\n",
    "    resultx, solx = solvex(x_guess, A, uvalvect)\n",
    "    resultlamb, sollamb = solvelamb(A, resultx, smallyvect, h)\n",
    "    gradient = jpufun(sollamb)\n",
    "    \n",
    "    # Define steepest descent move for new u_guess value\n",
    "    def steepest_descent_move(gradient, u_guess, step_size):\n",
    "        return u_guess - (step_size * gradient)\n",
    "    \n",
    "    while (iteration < maxiter):\n",
    "        gradient_mag = np.abs(jpufun(sollamb))\n",
    "        \n",
    "        # Output\n",
    "        if gradient_mag < tol:\n",
    "            print(f\"Converged after {iteration + 1} iterations.\")\n",
    "            print(f\"Optimal u value:\", u_guess)\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            u_guess = steepest_descent_move(gradient, u_guess, step_size)\n",
    "            u_guess_vect = u_guess * np.ones(N-1)\n",
    "            resultx, solx = solvex(x_guess, A, uvalvect)\n",
    "            resultlamb, sollamb = solvelamb(A, resultx, smallyvect, h)\n",
    "            gradient = jpufun(sollamb)\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "    if iteration == maxiter:\n",
    "        print(\"Maximum iterations reached without convergence.\")\n",
    "    \n",
    "    #return(osex, fsox, oseu, fsou)\n",
    "\n",
    "def plotspart1(osex, fsox):\n",
    "    # Plot comparison of aposteriori observation impact evaluated from OSEs and apriori FOS-based estimates\n",
    "    x_axis = np.linspace(1, 100, 100)\n",
    "    plt.plot(x_axis, osex, label='OSE (aposteriori)', marker='o')\n",
    "    plt.plot(x_axis, fsox, label='FSO (apriori)', marker='x')\n",
    "    plt.xlabel(f'i-th data component removed')\n",
    "    plt.ylabel(f'qoi(x) impact on removing i-th data component')\n",
    "    plt.title(f'OSE/FSO Observation Impact Estimates for qoi(x)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot relative error in apriori FOS-based estimates\n",
    "    x_axis = np.linspace(1, 100, 100)\n",
    "    plt.plot(x_axis, np.abs(fsox - osex) / np.abs(osex), marker='o')\n",
    "    plt.xlabel(f'i-th data component removed')\n",
    "    plt.ylabel(f'relative error in (apriori) FSO estimates')\n",
    "    plt.title(f'Relative Error in (apriori) FSO Observation Impact Estimates for qoi(x)')\n",
    "    plt.show()\n",
    "\n",
    "def plotspart2(oseu, fosu):\n",
    "    # Plot comparison of aposteriori observation impact evaluated from OSEs and apriori FOS-based estimates\n",
    "    x_axis = np.linspace(1, 100, 100)\n",
    "    plt.plot(x_axis, oseu, label='OSE (aposteriori)', marker='o')\n",
    "    plt.plot(x_axis, fsou, label='FSO (apriori)', marker='x')\n",
    "    plt.xlabel(f'i-th data component removed')\n",
    "    plt.ylabel(f'qoi(u) impact on removing i-th data component')\n",
    "    plt.title(f'OSE/FSO Observation Impact Estimates for qoi(u)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot relative error in apriori FOS-based estimates\n",
    "    x_axis = np.linspace(1, 100, 100)\n",
    "    plt.plot(x_axis, np.abs(fsou - oseu) / np.abs(oseu), marker='o')\n",
    "    plt.xlabel(f'i-th data component removed')\n",
    "    plt.ylabel(f'relative error in (apriori) FSO estimates')\n",
    "    plt.title(f'Relative Error in (apriori) FSO Observation Impact Estimates for qoi(u)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5c265-cc7b-4a3e-b903-70fbb0582d18",
   "metadata": {},
   "source": [
    "Furthermore, here are plots of the aposteriori observation impacts evaluated from OSEs and the apriori FSO-based estimates (12), and the relative error in the apriori estimates given the qoi based on $\\mathbf{x}^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d367398-59f4-4f97-b39a-64201db84987",
   "metadata": {},
   "outputs": [],
   "source": [
    "homework5(uguess, step_size)\n",
    "plotspart1(osex, fsox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8875fd-1190-40fa-9a76-088787d829d9",
   "metadata": {},
   "source": [
    "#### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8806bf-bbdd-4cdf-a2b5-0005df4cebfe",
   "metadata": {},
   "source": [
    "Here we are interested in apriori estimation of the impact of data removal on the optimal parameter value $u^*$. COndier the quantitiy of interest specified in terms of the parameter $u^*$ as \n",
    "$$\n",
    "Q(u) = \\frac{1}{2} |u-u^*|^2\n",
    "\\tag{23}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c901f-a132-43f5-a06e-19d6680aa2bb",
   "metadata": {},
   "source": [
    "The impact on the optimal parameter estimate produced by removing any individual data component $y_i, i=1:n$ from the optimization problem (3) can be quantified as \n",
    "$$\n",
    "\\Delta Q_i \\ \\dot{=} \\ Q(u_{[i]}^*)-Q(u^*)=\\frac{1}{2}|u_{[i]}^*-u^*|^2, i=1:n\n",
    "\\tag{24}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830b49c-0e90-43b8-a6ca-0d6dc4dd8769",
   "metadata": {},
   "source": [
    "The first thing we need to do is derive teh expression of the FSO-based apriori estimate of the impact of data removal (24)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000db07-df9d-43d6-9261-ab8ae919e788",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94ed38a7-e991-4814-9206-e03c114a0612",
   "metadata": {},
   "source": [
    "Furthermore, included are plots of the aposteriori observation impacts evaluated from OSEs (23) and the apriori FSO-based estimates (?), and the relative error in the apriori estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50698a0b-857d-4a50-be91-313bd8c95ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotspart2(oseu, fsou)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:optimization]",
   "language": "python",
   "name": "conda-env-optimization-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
